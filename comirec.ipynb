{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4583bdde-491c-494a-9b3f-9597ccb23bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss, NegativeSampler\n",
    "\n",
    "from collections import Counter\n",
    "import collections\n",
    "import faiss\n",
    "from deepmatch.utils import recall_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb73414-2dcb-4517-b71b-ed38a98d13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全量\n",
    "click_df = pd.read_csv('train_click_log.csv')\n",
    "click_df.columns = click_df.columns.str.strip()\n",
    "testa_df = pd.read_csv('testA_click_log.csv')\n",
    "testa_df.columns = testa_df.columns.str.strip()\n",
    "testb_df = pd.read_csv('testB_click_log.csv')\n",
    "testb_df.columns = testb_df.columns.str.strip()\n",
    "article_df = pd.read_csv('articles.csv')\n",
    "article_df.columns = article_df.columns.str.strip()\n",
    "\n",
    "merged_df = pd.concat([click_df, testa_df, testb_df], ignore_index=True)\n",
    "merged_df = pd.merge(merged_df, article_df, left_on='click_article_id', right_on='article_id', how='left')\n",
    "merged_df = merged_df.drop(columns=['article_id'])\n",
    "merged_df['click_timestamp'] = merged_df[['click_timestamp']].apply(lambda x : (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "\n",
    "test_df = pd.merge(testa_df, article_df, left_on='click_article_id', right_on='article_id', how='left')\n",
    "test_df['click_timestamp'] = test_df[['click_timestamp']].apply(lambda x : (x-np.min(x))/(np.max(x)-np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a4886e-747c-4024-961f-ff60a8f05217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 获取双塔召回时的训练验证数据\n",
    "# negsample指的是通过滑窗构建样本的时候，负样本的数量\n",
    "def gen_data_set(data, seq_max_len=10, negsample=0):\n",
    "    # 用户id，新闻id，正负样本：1 or 0，[历史点击序列]，序列长度，[历史点击序列类目]，新闻id类目，新闻id词数, [历史点击序列词数]\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "    item_ids = data['click_article_id'].unique()\n",
    "\n",
    "    item_id_cateory_map = dict(zip(data['click_article_id'].values, data['category_id'].values))\n",
    "    wordcount_id_cateory_map = dict(zip(data['click_article_id'].values, data['words_count'].values))\n",
    "    \n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['click_article_id'].tolist()\n",
    "        category_list = hist['category_id'].tolist()\n",
    "        wordcount_list = hist['words_count'].tolist()\n",
    "        \n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))   # 用户没看过的文章里面选择负样本\n",
    "            neg_list = np.random.choice(candidate_set, size=len(pos_list) * negsample, replace=True)  # 对于每个正样本，选择n个负样本\n",
    "            \n",
    "        # 长度只有一个的时候，需要把这条数据也放到训练集中，不然的话最终学到的embedding就会有缺失\n",
    "        # 同时label decode会不成功，因为有些数据就没被encode\n",
    "        if len(pos_list) == 1:\n",
    "            hist = pos_list[0]\n",
    "            category_hist = category_list[0]\n",
    "            wordcount_hist = wordcount_list[0]\n",
    "            \n",
    "            train_set.append((reviewerID, \n",
    "                                pos_list[0], \n",
    "                                1, \n",
    "                                [hist], \n",
    "                                seq_len, \n",
    "                                [category_hist],\n",
    "                                category_list[0],\n",
    "                                wordcount_list[0],\n",
    "                                [wordcount_hist])) \n",
    "\n",
    "        # 全量从0开始\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            category_hist = category_list[:i]\n",
    "            wordcount_hist = wordcount_list[:i]\n",
    "            seq_len = min(i, seq_max_len)\n",
    "            \n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((\n",
    "                    reviewerID, \n",
    "                    pos_list[i], \n",
    "                    1, \n",
    "                    hist[::-1][:seq_len], \n",
    "                    seq_len, \n",
    "                    category_hist[::-1][:seq_len],\n",
    "                    category_list[i],\n",
    "                    wordcount_list[i],\n",
    "                    wordcount_hist[::-1][:seq_len]))\n",
    "                \n",
    "                # 负样本\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, \n",
    "                                      neg_list[i * negsample + negi], \n",
    "                                      0, \n",
    "                                      hist[::-1][:seq_len], \n",
    "                                      seq_len,\n",
    "                                      category_hist[::-1][:seq_len], \n",
    "                                      item_id_cateory_map[neg_list[i * negsample + negi]], \n",
    "                                      wordcount_id_cateory_map[neg_list[i * negsample + negi]],\n",
    "                                      wordcount_hist[::-1][:seq_len]))\n",
    "            \n",
    "            else:\n",
    "                # 测试集\n",
    "                test_set.append((\n",
    "                                 reviewerID, \n",
    "                                 pos_list[i], \n",
    "                                 1, \n",
    "                                 hist[::-1][:seq_len], \n",
    "                                 seq_len, \n",
    "                                 category_hist[::-1][:seq_len],\n",
    "                                 category_list[i],\n",
    "                                 wordcount_list[i],\n",
    "                                 wordcount_hist[::-1][:seq_len]))       \n",
    "            \n",
    "            \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    return train_set, test_set\n",
    "    # 用户id，新闻id，正负样本：1 or 0，[历史点击序列]，序列长度，[历史点击序列类目]，新闻id类目，新闻id词数, [历史点击序列词数]\n",
    "\n",
    "\n",
    "# 将输入的数据进行padding，使得序列特征的长度都一致\n",
    "# userprofile = [\"user_id\", 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\n",
    "# itemprofile = [\"category_id\", \"words_count\"]\n",
    "def gen_model_input(train_set, user_profile, seq_max_len):\n",
    "    # 0       1       2                 3               4         5                   6           7\n",
    "    # 用户id，新闻id，正负样本：1 or 0，[历史点击序列]，序列长度，[历史点击序列类目]，新闻id类目，新闻id词数\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_iid = np.array([line[1] for line in train_set])\n",
    "    train_label = np.array([line[2] for line in train_set])\n",
    "    train_seq = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "    train_category_seq = np.array([line[5] for line in train_set])\n",
    "    train_category = np.array([line[6] for line in train_set])\n",
    "    train_wordcount = np.array([line[7] for line in train_set])\n",
    "    train_wordcount_seq = np.array([line[8] for line in train_set])\n",
    "    \n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_seq_category_pad = pad_sequences(train_category_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_seq_wordcount_pad = pad_sequences(train_wordcount_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    \n",
    "    train_model_input = {'user_id': train_uid,\n",
    "                         'click_article_id': train_iid,\n",
    "                         'hist_click_article_id': train_seq_pad,\n",
    "                         'hist_category_id': train_seq_category_pad,\n",
    "                         'hist_len': train_hist_len,\n",
    "                         'category': train_category,\n",
    "                         'wordcount': train_wordcount,\n",
    "                         'hist_words_count': train_seq_wordcount_pad}\n",
    "    \n",
    "    for key in ['click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc62bbd-bb48-4aa5-b521-f69e875c2bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu-22-04-03/anaconda3/envs/recsys/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "data = merged_df.copy()\n",
    "\n",
    "sparse_features = [\"user_id\", \"click_article_id\",\n",
    "                    'click_environment', 'click_deviceGroup', \n",
    "                    'click_os', 'click_country', 'click_region', \n",
    "                    'click_referrer_type', 'category_id',\n",
    "                    'words_count']\n",
    "SEQ_LEN = 10 # 用户点击序列的长度，短的填充，长的截断\n",
    "negsample = 0\n",
    "\n",
    "# 类别编码\n",
    "# NOTE：这个label_encoders = {}是为了记录每一个特征的label encoder\n",
    "# 到时候解码的时候可以用对应的特征编码器来解码。\n",
    "label_encoders = {}\n",
    "feature_max_idx = {}\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    # 如果包含了只有len为1的数据的时候，这里就不要+1了\n",
    "    data[feature] = lbe.fit_transform(data[feature])\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "    # 逆转编码\n",
    "    label_encoders[feature] = lbe # 保存对每一个特征的编码器\n",
    "    \n",
    "    \n",
    "user_profile = data[[\"user_id\", 'click_environment', \n",
    "                    'click_deviceGroup', 'click_os', \n",
    "                    'click_country', 'click_region', \n",
    "                    'click_referrer_type']].drop_duplicates('user_id')\n",
    "\n",
    "item_profile = data[[\"click_article_id\",\n",
    "                     'category_id',\n",
    "                     'words_count']].drop_duplicates('click_article_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True, drop=False)\n",
    "\n",
    "user_item_list = data.groupby(\"user_id\")['click_article_id'].apply(list)\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "                        SparseFeat('click_environment', feature_max_idx['click_environment'], 16),\n",
    "                        SparseFeat('click_deviceGroup', feature_max_idx['click_deviceGroup'], 16),\n",
    "                        SparseFeat('click_country', feature_max_idx['click_country'], 16),\n",
    "                        SparseFeat('click_region', feature_max_idx['click_region'], 16),\n",
    "                        SparseFeat('click_referrer_type', feature_max_idx['click_referrer_type'], 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_click_article_id', feature_max_idx['click_article_id'], embedding_dim,\n",
    "                                                     embedding_name='article_id'), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_category_id', feature_max_idx['category_id'], embedding_dim,\n",
    "                                                     embedding_name='category_id'), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_words_count', feature_max_idx['words_count'], embedding_dim,\n",
    "                                embedding_name='words_count'), SEQ_LEN, 'mean', 'hist_len')\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('click_article_id', feature_max_idx['click_article_id'], embedding_dim),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07237554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:49<00:00, 6107.21it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = gen_data_set(data, SEQ_LEN, negsample)\n",
    "\n",
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f09c0bb2-928f-4485-9e10-89723dec23c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1841158 samples\n",
      "Epoch 1/5\n",
      "1841158/1841158 [==============================] - 107s 58us/sample - loss: 4.6601\n",
      "Epoch 2/5\n",
      "1841158/1841158 [==============================] - 95s 51us/sample - loss: 3.8881\n",
      "Epoch 3/5\n",
      "1841158/1841158 [==============================] - 86s 47us/sample - loss: 3.5834\n",
      "Epoch 4/5\n",
      "1841158/1841158 [==============================] - 94s 51us/sample - loss: 3.3813\n",
      "Epoch 5/5\n",
      "1841158/1841158 [==============================] - 103s 56us/sample - loss: 3.2284\n"
     ]
    }
   ],
   "source": [
    "train_counter = Counter(train_model_input['click_article_id'])\n",
    "item_count = [train_counter.get(i, 0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "sampler_config_youtubednn = NegativeSampler('frequency', num_sampled=255, item_name=\"click_article_id\", item_count=item_count)\n",
    "# model = YoutubeDNN(user_feature_columns, item_feature_columns, user_dnn_hidden_units=(128,64, embedding_dim), sampler_config=sampler_config)\n",
    "comirec = ComiRec(user_feature_columns, item_feature_columns, k_max=2, user_dnn_hidden_units=(64, embedding_dim), sampler_config=sampler_config_youtubednn)\n",
    "\n",
    "comirec.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n",
    "\n",
    "history = comirec.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=512, epochs=5, verbose=1, validation_split=0.0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "823e24be-475d-4ae9-b3c5-037c3b613c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记得类别编码\n",
    "testa_df = pd.read_csv('testA_click_log.csv')\n",
    "testa_df.columns = testa_df.columns.str.strip()\n",
    "\n",
    "article_df = pd.read_csv('articles.csv')\n",
    "article_df.columns = article_df.columns.str.strip()\n",
    "\n",
    "testa_df = pd.merge(testa_df, article_df, left_on='click_article_id', right_on='article_id', how='left')\n",
    "testa_df['click_timestamp'] = testa_df[['click_timestamp']].apply(lambda x : (x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "\n",
    "\n",
    "# NOTE: 这里想要做一个全量数据的，就是把最后一个点击也包括进去的那种\n",
    "def gen_train_set_no_shuffle(data, seq_max_len=10, negsample=0):\n",
    "    # 用户id，新闻id，正负样本：1 or 0，[历史点击序列]，序列长度，[历史点击序列类目]，新闻id类目，新闻id词数, [历史点击序列词数]\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "    item_ids = data['click_article_id'].unique()\n",
    "\n",
    "    item_id_cateory_map = dict(zip(data['click_article_id'].values, data['category_id'].values))\n",
    "    wordcount_id_cateory_map = dict(zip(data['click_article_id'].values, data['words_count'].values))\n",
    "    \n",
    "    train_set = []\n",
    "    \n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['click_article_id'].tolist()\n",
    "        category_list = hist['category_id'].tolist()\n",
    "        wordcount_list = hist['words_count'].tolist()\n",
    "        \n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))   # 用户没看过的文章里面选择负样本\n",
    "            neg_list = np.random.choice(candidate_set, size=len(pos_list) * negsample, replace=True)  # 对于每个正样本，选择n个负样本\n",
    "            \n",
    "        # 长度只有一个的时候，需要把这条数据也放到训练集中，不然的话最终学到的embedding就会有缺失\n",
    "        # 同时label decode会不成功，因为有些数据就没被encode\n",
    "        if len(pos_list) == 1:\n",
    "            hist = pos_list[0]\n",
    "            category_hist = category_list[0]\n",
    "            wordcount_hist = wordcount_list[0]\n",
    "            \n",
    "            train_set.append((reviewerID, \n",
    "                                pos_list[0], \n",
    "                                1, \n",
    "                                [hist], \n",
    "                                seq_len, \n",
    "                                [category_hist],\n",
    "                                category_list[0],\n",
    "                                wordcount_list[0],\n",
    "                                [wordcount_hist])) \n",
    "\n",
    "        # 全量从0开始\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            category_hist = category_list[:i]\n",
    "            wordcount_hist = wordcount_list[:i]\n",
    "            seq_len = min(i, seq_max_len)\n",
    "            \n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((\n",
    "                    reviewerID, \n",
    "                    pos_list[i], \n",
    "                    1, \n",
    "                    hist[::-1][:seq_len], \n",
    "                    seq_len, \n",
    "                    category_hist[::-1][:seq_len],\n",
    "                    category_list[i],\n",
    "                    wordcount_list[i],\n",
    "                    wordcount_hist[::-1][:seq_len]))\n",
    "                \n",
    "                # 负样本\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, \n",
    "                                      neg_list[i * negsample + negi], \n",
    "                                      0, \n",
    "                                      hist[::-1][:seq_len], \n",
    "                                      seq_len,\n",
    "                                      category_hist[::-1][:seq_len], \n",
    "                                      item_id_cateory_map[neg_list[i * negsample + negi]], \n",
    "                                      wordcount_id_cateory_map[neg_list[i * negsample + negi]],\n",
    "                                      wordcount_hist[::-1][:seq_len]))\n",
    "            else:\n",
    "                # 测试集\n",
    "                train_set.append((\n",
    "                                 reviewerID, \n",
    "                                 pos_list[i], \n",
    "                                 1, \n",
    "                                 hist[::-1][:seq_len], \n",
    "                                 seq_len, \n",
    "                                 category_hist[::-1][:seq_len],\n",
    "                                 category_list[i],\n",
    "                                 wordcount_list[i],\n",
    "                                 wordcount_hist[::-1][:seq_len]))\n",
    "            \n",
    "            \n",
    "    random.shuffle(train_set)\n",
    "    \n",
    "    return train_set\n",
    "\n",
    "\n",
    "\n",
    "# 用来预测新的测试集\n",
    "# NOTE: 这个出来的历史点击序列包含所有历史点击，而不是之前的把最后一个拿出来\n",
    "def gen_pred_set(data, seq_max_len=10):\n",
    "    # 用户id，新闻id，正负样本：1 or 0，[历史点击序列]，序列长度，[历史点击序列类目]，新闻id类目，新闻id词数, [历史点击序列词数]\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "\n",
    "    test_set = []\n",
    "    \n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['click_article_id'].tolist()\n",
    "        category_list = hist['category_id'].tolist()\n",
    "        wordcount_list = hist['words_count'].tolist()\n",
    "        \n",
    "        hist = pos_list[:len(pos_list)]\n",
    "        category_hist = category_list[:len(pos_list)]\n",
    "        wordcount_hist = wordcount_list[:len(pos_list)]\n",
    "        seq_len = min(len(pos_list), seq_max_len)\n",
    "        \n",
    "        test_set.append((\n",
    "                    reviewerID, \n",
    "                    pos_list[-1], \n",
    "                    1, \n",
    "                    hist[::-1][:seq_len], \n",
    "                    seq_len, \n",
    "                    category_hist[::-1][:seq_len],\n",
    "                    category_list[-1],\n",
    "                    wordcount_list[-1],\n",
    "                    wordcount_hist[::-1][:seq_len]))   \n",
    "          \n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "\n",
    "# NOTE: 这个出来的是n - 1个历史点击，新闻id是最后一个点击的\n",
    "def gen_test_pred_set(data, seq_max_len=10):\n",
    "    # 用户id，新闻id，正负样本：1 or 0，[历史点击序列]，序列长度，[历史点击序列类目]，新闻id类目，新闻id词数, [历史点击序列词数]\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "\n",
    "    test_set = []\n",
    "    \n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['click_article_id'].tolist()\n",
    "        category_list = hist['category_id'].tolist()\n",
    "        wordcount_list = hist['words_count'].tolist()\n",
    "        \n",
    "        hist = pos_list[:len(pos_list) - 1]\n",
    "        category_hist = category_list[:len(pos_list) - 1]\n",
    "        wordcount_hist = wordcount_list[:len(pos_list) - 1]\n",
    "        seq_len = min(len(pos_list), seq_max_len)\n",
    "        \n",
    "        test_set.append((\n",
    "                    reviewerID, \n",
    "                    pos_list[-1], \n",
    "                    1, \n",
    "                    hist[::-1][:seq_len], \n",
    "                    seq_len, \n",
    "                    category_hist[::-1][:seq_len],\n",
    "                    category_list[-1],\n",
    "                    wordcount_list[-1],\n",
    "                    wordcount_hist[::-1][:seq_len]))   \n",
    "          \n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "\n",
    "# NOTE：用testA来预测\n",
    "testa_df = pd.read_csv('testA_click_log.csv')\n",
    "testa_df.columns = testa_df.columns.str.strip()\n",
    "\n",
    "testa_df = pd.merge(testa_df, article_df, left_on='click_article_id', right_on='article_id', how='left')\n",
    "testa_df = testa_df.drop(columns=['article_id'])\n",
    "\n",
    "# 测试集编码\n",
    "for feature in sparse_features:\n",
    "    if feature in label_encoders:\n",
    "        lbe = label_encoders[feature]\n",
    "        testa_df[feature] = lbe.transform(testa_df[feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dffc63cd-f9eb-43fd-b60d-5f487ea0a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "693cc650-3727-479c-9a89-cb08515d285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283066, 2, 16)\n",
      "(41047, 16)\n"
     ]
    }
   ],
   "source": [
    "# 4. Generate user features for testing and full item features for retrieval\n",
    "test_user_model_input = test_model_input\n",
    "all_item_model_input = {\"click_article_id\": item_profile['click_article_id'].values,}\n",
    "\n",
    "user_embedding_model = Model(inputs=comirec.user_input, outputs=comirec.user_embedding)\n",
    "item_embedding_model = Model(inputs=comirec.item_input, outputs=comirec.item_embedding)\n",
    "\n",
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b84516f-3fff-465b-b9e5-5ae6de3de8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283066/283066 [00:06<00:00, 41271.93it/s]\n",
      "100%|██████████| 283066/283066 [00:05<00:00, 51600.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.24987105480700614\n",
      "hr 0.24987105480700614\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "k_max = 2\n",
    "topN = 5\n",
    "test_true_label = {line[0]: [line[1]] for line in test_set}\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)\n",
    "# faiss.normalize_L2(user_embs)\n",
    "\n",
    "retrieval_items = defaultdict(list)\n",
    "\n",
    "if len(user_embs.shape) == 2:  # multi interests model's shape = 3 (MIND,ComiRec)\n",
    "    user_embs = np.expand_dims(user_embs, axis=1)\n",
    "\n",
    "score_dict = defaultdict(dict)\n",
    "for k in range(k_max):\n",
    "    user_emb = user_embs[:, k, :]\n",
    "    D, I = index.search(np.ascontiguousarray(user_emb), topN)\n",
    "    for i, uid in tqdm(enumerate(test_user_model_input['user_id']), total=len(test_user_model_input['user_id'])):\n",
    "        if np.abs(user_emb[i]).max() < 1e-8:\n",
    "            continue\n",
    "        for score, itemid in zip(D[i], I[i]):\n",
    "            score_dict[uid][itemid] = max(score, score_dict[uid].get(itemid, float(\"-inf\")))\n",
    "\n",
    "s = []\n",
    "hit = 0\n",
    "for i, uid in enumerate(test_user_model_input['user_id']):\n",
    "    pred = [item_profile['click_article_id'].values[x[0]] for x in\n",
    "            heapq.nlargest(topN, score_dict[uid].items(), key=lambda x: x[1])]\n",
    "    filter_item = None\n",
    "    recall_score = recall_N(test_true_label[uid], pred, N=topN)\n",
    "    s.append(recall_score)\n",
    "    if test_true_label[uid] in pred:\n",
    "        hit += 1\n",
    "    \n",
    "    retrieval_items[uid] = pred\n",
    "\n",
    "print(\"recall\", np.mean(s))\n",
    "print(\"hr\", hit / len(test_user_model_input['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8da5a27-54c3-49cc-8322-0b26262379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把召回出来的dic，转换成df （label encoded）\n",
    "retrieval_items_df = pd.DataFrame.from_dict(retrieval_items, orient='index', columns=['article_1', 'article_2', 'article_3', 'article_4', 'article_5'])\n",
    "\n",
    "# 重置索引并将索引设置为user_id\n",
    "retrieval_items_df.reset_index(inplace=True)\n",
    "retrieval_items_df.rename(columns={'index': 'user_id'}, inplace=True)\n",
    "\n",
    "# 测试用户最后一次点击的文章列表（label encoded）\n",
    "test_true_label_df = pd.DataFrame([(user_id, article_id[0]) for user_id, article_id in test_true_label.items()], columns=['user_id', 'click_article_id'])\n",
    "\n",
    "# 为了计算MRR和HR，将召回文章列表和真实点击的文章列表merge到一起\n",
    "merged_retrieval_true_label_df = pd.merge(retrieval_items_df, test_true_label_df, on='user_id')\n",
    "\n",
    "# 用记录下来的每一个特征的label encoder，来decode数据\n",
    "id_encoder = label_encoders['user_id']\n",
    "merged_retrieval_true_label_df['user_id'] = id_encoder.inverse_transform(merged_retrieval_true_label_df['user_id'])\n",
    "\n",
    "# 愚蠢但有用\n",
    "article_id_encoder = label_encoders['click_article_id']\n",
    "merged_retrieval_true_label_df['article_1'] = article_id_encoder.inverse_transform(merged_retrieval_true_label_df['article_1'])\n",
    "merged_retrieval_true_label_df['article_2'] = article_id_encoder.inverse_transform(merged_retrieval_true_label_df['article_2'])\n",
    "merged_retrieval_true_label_df['article_3'] = article_id_encoder.inverse_transform(merged_retrieval_true_label_df['article_3'])\n",
    "merged_retrieval_true_label_df['article_4'] = article_id_encoder.inverse_transform(merged_retrieval_true_label_df['article_4'])\n",
    "merged_retrieval_true_label_df['article_5'] = article_id_encoder.inverse_transform(merged_retrieval_true_label_df['article_5'])\n",
    "merged_retrieval_true_label_df['click_article_id'] = article_id_encoder.inverse_transform(merged_retrieval_true_label_df['click_article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dae3f2d9-0574-4c13-bee3-81d21b0577d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.1400512012510611\n"
     ]
    }
   ],
   "source": [
    "# 计算MRR\n",
    "def reciprocal_rank(row):\n",
    "    recommended_articles = [row[f'article_{i}'] for i in range(1, 6)]\n",
    "    try:\n",
    "        rank = recommended_articles.index(row['click_article_id']) + 1\n",
    "        return 1 / rank\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "merged_retrieval_true_label_df['reciprocal_rank'] = merged_retrieval_true_label_df.apply(reciprocal_rank, axis=1)\n",
    "\n",
    "# 计算MRR\n",
    "mrr = merged_retrieval_true_label_df['reciprocal_rank'].mean()\n",
    "print(f'MRR: {mrr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ca81d0-29e1-47ee-b445-733155c725cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 0.24987105480700614\n"
     ]
    }
   ],
   "source": [
    "# 计算hit rate\n",
    "def hit_rate(row):\n",
    "    recommended_articles = [row[f'article_{i}'] for i in range(1, 6)]\n",
    "    return int(row['click_article_id'] in recommended_articles)\n",
    "\n",
    "merged_retrieval_true_label_df['hit'] = merged_retrieval_true_label_df.apply(hit_rate, axis=1)\n",
    "\n",
    "# 计算Hit Rate\n",
    "hit_rate_value = merged_retrieval_true_label_df['hit'].mean()\n",
    "print(f'Hit Rate: {hit_rate_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6281aa-e281-49c7-8ccb-bc9124c0468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu-22-04-03/anaconda3/envs/recsys/lib/python3.7/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_1</th>\n",
       "      <th>article_2</th>\n",
       "      <th>article_3</th>\n",
       "      <th>article_4</th>\n",
       "      <th>article_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145702</th>\n",
       "      <td>0</td>\n",
       "      <td>331116</td>\n",
       "      <td>277107</td>\n",
       "      <td>177442</td>\n",
       "      <td>336254</td>\n",
       "      <td>235105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209477</th>\n",
       "      <td>1</td>\n",
       "      <td>331116</td>\n",
       "      <td>289003</td>\n",
       "      <td>336254</td>\n",
       "      <td>157478</td>\n",
       "      <td>199372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137270</th>\n",
       "      <td>2</td>\n",
       "      <td>50644</td>\n",
       "      <td>36162</td>\n",
       "      <td>70986</td>\n",
       "      <td>331116</td>\n",
       "      <td>277107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170271</th>\n",
       "      <td>3</td>\n",
       "      <td>50644</td>\n",
       "      <td>36162</td>\n",
       "      <td>70986</td>\n",
       "      <td>209122</td>\n",
       "      <td>206415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201567</th>\n",
       "      <td>4</td>\n",
       "      <td>42757</td>\n",
       "      <td>69129</td>\n",
       "      <td>50494</td>\n",
       "      <td>159019</td>\n",
       "      <td>73506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117883</th>\n",
       "      <td>299994</td>\n",
       "      <td>160974</td>\n",
       "      <td>161178</td>\n",
       "      <td>50504</td>\n",
       "      <td>157077</td>\n",
       "      <td>273616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196634</th>\n",
       "      <td>299995</td>\n",
       "      <td>336220</td>\n",
       "      <td>158794</td>\n",
       "      <td>289090</td>\n",
       "      <td>202388</td>\n",
       "      <td>233688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36688</th>\n",
       "      <td>299997</td>\n",
       "      <td>284410</td>\n",
       "      <td>160131</td>\n",
       "      <td>168623</td>\n",
       "      <td>323550</td>\n",
       "      <td>286321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216353</th>\n",
       "      <td>299998</td>\n",
       "      <td>336223</td>\n",
       "      <td>119193</td>\n",
       "      <td>271261</td>\n",
       "      <td>48403</td>\n",
       "      <td>59057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141549</th>\n",
       "      <td>299999</td>\n",
       "      <td>183176</td>\n",
       "      <td>128260</td>\n",
       "      <td>111043</td>\n",
       "      <td>215993</td>\n",
       "      <td>20691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283066 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  article_1  article_2  article_3  article_4  article_5\n",
       "145702        0     331116     277107     177442     336254     235105\n",
       "209477        1     331116     289003     336254     157478     199372\n",
       "137270        2      50644      36162      70986     331116     277107\n",
       "170271        3      50644      36162      70986     209122     206415\n",
       "201567        4      42757      69129      50494     159019      73506\n",
       "...         ...        ...        ...        ...        ...        ...\n",
       "117883   299994     160974     161178      50504     157077     273616\n",
       "196634   299995     336220     158794     289090     202388     233688\n",
       "36688    299997     284410     160131     168623     323550     286321\n",
       "216353   299998     336223     119193     271261      48403      59057\n",
       "141549   299999     183176     128260     111043     215993      20691\n",
       "\n",
       "[283066 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = merged_retrieval_true_label_df[['user_id', 'article_1', 'article_2', 'article_3', 'article_4', 'article_5']]\n",
    "result_df.sort_values(by='user_id', inplace=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b0ad95-b308-46cf-812d-0b9d9f7c07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('ans_comirec.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
